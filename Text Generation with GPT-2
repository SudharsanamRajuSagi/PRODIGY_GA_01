Load the Pre-trained Model:

from transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")



Fine-tune the Model:

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=1000,
    evaluation_strategy="steps",
    logging_steps=1000,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

trainer.train()




Save the Fine-tuned Model:

trainer.save_model("fine_tuned_gpt2")



prompt = "Once upon a time, in a land far away..."
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(inputs, max_length=100, num_beams=5)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_text)
